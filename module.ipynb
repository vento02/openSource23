{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtyb2m5e8FYc"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "from transformers import BartForConditionalGeneration\n",
        "\n",
        "\n",
        "\n",
        "class StoryModule(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Attributes:\n",
        "        model: BART model\n",
        "        total_steps: total training steps for lr scheduling\n",
        "        max_learning_rate: Max LR\n",
        "        min_learning_rate: Min LR\n",
        "        warmup_rate: warmup step rate\n",
        "        model_save_dir: path to save model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: BartForConditionalGeneration,\n",
        "        total_steps: int,\n",
        "        max_learning_rate: float,\n",
        "        min_learning_rate: float,\n",
        "        warmup_rate: float,\n",
        "        model_save_dir: str,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.total_steps = total_steps\n",
        "        self.max_learning_rate = max_learning_rate\n",
        "        self.min_learning_rate = min_learning_rate\n",
        "        self.warmup_rate = warmup_rate\n",
        "        self.model_save_dir = model_save_dir\n",
        "\n",
        "        self.save_hyperparameters(\n",
        "            {\n",
        "                **model.config.to_dict(),\n",
        "                \"total_steps\": total_steps,\n",
        "                \"max_learning_rate\": self.max_learning_rate,\n",
        "                \"min_learning_rate\": self.min_learning_rate,\n",
        "                \"warmup_rate\": self.warmup_rate,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        output = self.model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            decoder_input_ids=batch[\"decoder_input_ids\"],\n",
        "            decoder_attention_mask=batch[\"decoder_attention_mask\"],\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        labels = batch[\"decoder_input_ids\"][:, 1:].reshape(-1)\n",
        "        logits = output[\"logits\"][:, :-1].reshape([labels.shape[0], -1])\n",
        "\n",
        "        loss = F.cross_entropy(logits, labels, ignore_index=self.model.config.pad_token_id)\n",
        "        accuracy = torchmetrics.functional.accuracy(logits, labels, ignore_index=self.model.config.pad_token_id)\n",
        "\n",
        "        metrics = {\"loss\": loss, \"acc\": accuracy}\n",
        "        self.log_dict(metrics, prog_bar=True, logger=True, on_step=True)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        output = self.model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            decoder_input_ids=batch[\"decoder_input_ids\"],\n",
        "            decoder_attention_mask=batch[\"decoder_attention_mask\"],\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        labels = batch[\"decoder_input_ids\"][:, 1:].reshape(-1)\n",
        "        logits = output[\"logits\"][:, :-1].reshape([labels.shape[0], -1])\n",
        "\n",
        "        loss = F.cross_entropy(logits, labels, ignore_index=self.model.config.pad_token_id)\n",
        "        accuracy = torchmetrics.functional.accuracy(logits, labels, ignore_index=self.model.config.pad_token_id)\n",
        "\n",
        "        metrics = {\"val_loss\": loss, \"val_acc\": accuracy}\n",
        "        self.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def test_step(self, *args, **kwargs):\n",
        "        return self.validation_step(*args, **kwargs)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(params=self.model.parameters(), lr=self.max_learning_rate)\n",
        "        scheduler = CyclicLR(\n",
        "            optimizer,\n",
        "            base_lr=self.min_learning_rate,\n",
        "            max_lr=self.max_learning_rate,\n",
        "            step_size_up=int(self.total_steps * self.warmup_rate),\n",
        "            step_size_down=self.total_steps - int(self.total_steps * self.warmup_rate),\n",
        "            mode='triangular',\n",
        "            cycle_momentum=False\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\", \"name\": \"Learning Rate\"},\n",
        "        }\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        outputs = self.all_gather(outputs)\n",
        "\n",
        "        if self.trainer.is_global_zero:\n",
        "            val_losses = [output[\"val_loss\"].mean() for output in outputs]\n",
        "            val_accs = [output[\"val_acc\"].mean() for output in outputs]\n",
        "\n",
        "            val_loss_mean = sum(val_losses) / len(val_losses)\n",
        "            val_acc_mean = sum(val_accs) / len(val_accs)\n",
        "\n",
        "            self.model.save_pretrained(\n",
        "                os.path.join(\n",
        "                    self.model_save_dir,\n",
        "                    f\"model-{self.current_epoch:02d}epoch-{self.global_step}steps-{val_loss_mean:.4f}loss-{val_acc_mean:.4f}acc\",\n",
        "                ),\n",
        "            )\n"
      ]
    }
  ]
}